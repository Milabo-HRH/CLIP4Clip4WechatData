# CLIP4CCï¼šCLIP for Clip Classification

### Wechat

```sh
python -m torch.distributed.launch --nproc_per_node=1 main_task_retrieval.py --do_train --num_thread_reader=36 --epochs=200 --batch_size=4096 --n_display=50 --data_path ./data/annotations --features_path ./data/zip_feats/pretrain.zip --output_dir ckpts/ckpt_wechat_retrieval_looseType --lr 5e-4 --max_words 96 --max_frames 32 --batch_size_val 16 --datatype wechat --feature_framerate 1 --coef_lr 1e-3 --freeze_layer_num 0  --slice_framepos 2 --save_epoch 10 --loose_type --linear_patch 2d --sim_header meanP --pretrained_clip_name ./modules/clip_cn_vit-b-16.pt
python -m torch.distributed.launch --nproc_per_node=1 main_task_retrieval.py --do_finetune --num_thread_reader=36 --epochs=50 --batch_size=128 --n_display=1000 --data_path ./data/annotations --features_path ./data/zip_feats/pretrain.zip --output_dir ckpts/ckpt_wechat_retrieval_looseType --lr 5e-4 --max_words 96 --max_frames 32 --batch_size_val 16 --datatype wechat --feature_framerate 1 --coef_lr 1e-3 --freeze_layer_num 0  --slice_framepos 2 --save_epoch 10 --loose_type --linear_patch 2d --sim_header meanP --pretrained_clip_name ./modules/clip_cn_vit-b-16.pt --no_clip
python -m torch.distributed.launch --nproc_per_node=1 main_task_retrieval.py --do_finetune --num_thread_reader=36 --epochs=5 --batch_size=128 --n_display=50 --data_path ./data/annotations --features_path ./data/zip_feats/pretrain.zip --output_dir ckpts/ckpt_wechat_retrieval_looseType --lr 5e-4 --max_words 96 --max_frames 32 --batch_size_val 16 --datatype wechat --feature_framerate 1 --coef_lr 1e-3 --freeze_layer_num 0  --slice_framepos 2 --save_epoch 5 --loose_type --linear_patch 2d --sim_header meanP --pretrained_clip_name ./modules/clip_cn_vit-b-16.pt --res ckpts/ckpt_wechat_retrieval_looseType/best_perform.bin --load_finetune ckpts/ckpt_wechat_retrieval_looseType/pytorch_model.bin.finetuning.49 --do_ssl --only_positive
python -m torch.distributed.launch --nproc_per_node=1 main_eval.py --do_finetune --num_thread_reader=36 --epochs=50 --batch_size=128 --n_display=50 --data_path ./data/annotations --features_path ./data/zip_feats/pretrain.zip --output_dir ckpts/ckpt_wechat_retrieval_looseType --lr 5e-4 --max_words 32 --max_frames 32 --batch_size_val 16 --datatype wechat --feature_framerate 1 --coef_lr 1e-3 --freeze_layer_num 0  --slice_framepos 2 --save_epoch 5 --loose_type --linear_patch 2d --sim_header meanP --pretrained_clip_name ./modules/clip_cn_vit-b-16.pt --res ckpts/ckpt_wechat_retrieval_looseType/best_perform.bin --load_finetune ckpts/ckpt_wechat_retrieval_looseType/pytorch_model.bin.finetuning.34m_thread_reader=36 --epochs=50 --batch_size=128 --n_display=50 --data_path ./data/annotations --features_path ./data/zip_feats/pretrain.zip --output_dir ckpts/ckpt_wechat_retrieval_looseType --lr 5e-4 --max_words 32 --max_frames 32 --batch_size_val 16 --datatype wechat --feature_framerate 1 --coef_lr 1e-3 --freeze_layer_num 0  --slice_framepos 2 --save_epoch 5 --loose_type --linear_patch 2d --sim_header meanP --pretrained_clip_name ./modules/clip_cn_vit-b-16.pt --res ckpts/ckpt_wechat_retrieval_looseType/best_perform.bin --load_finetune ckpts/ckpt_wechat_retrieval_looseType/pytorch_model.bin.finetuning.4 --resume_model ckpts/ckpt_wechat_retrieval_looseType/pytorch_opt.bin.finetuning.4

python -m torch.distributed.launch --nproc_per_node=1 main_eval.py --do_finetune --num_thread_reader=36 --epochs=50 --batch_size=128 --n_display=50 --data_path ./data/annotations --features_path ./data/zip_feats/pretrain.zip --output_dir ckpts/ckpt_wechat_retrieval_looseType --lr 5e-4 --max_words 32 --max_frames 32 --batch_size_val 16 --datatype wechat --feature_framerate 1 --coef_lr 1e-3 --freeze_layer_num 0  --slice_framepos 2 --save_epoch 5 --loose_type --linear_patch 2d --sim_header meanP --pretrained_clip_name ./modules/clip_cn_vit-b-16.pt --res ckpts/ckpt_wechat_retrieval_looseType/best_perform.bin --load_finetune ckpts/ckpt_wechat_retrieval_looseType/pytorch_model.bin.finetuning.3
```
